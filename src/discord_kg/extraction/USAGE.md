# Step 3: Entity & Relation Extraction - Usage Guide

This directory contains two extraction implementations organized in separate subdirectories:

## üîß Implementation Options

### 1. **Rule-Based Extractor** (`extractor.py`)
- **Cost**: ~$10-30 for 1M messages (local processing)
- **Speed**: Fast (100+ messages/second)
- **Accuracy**: 85-95% for structured patterns
- **Best for**: Production deployment, cost-sensitive use cases

### 2. **LLM-Powered Extractor** (`extractor_llm.py`) 
- **Cost**: ~$150-500 for 1M messages (API calls)
- **Speed**: Medium (depends on API limits)
- **Accuracy**: 90-95% for complex reasoning
- **Best for**: High-accuracy requirements, complex analysis

## üöÄ Quick Start

### Rule-Based Extraction (`rule_based/`)
```bash
cd rule_based/

# Install dependencies
pip install -r requirements.txt

# Run extraction
python extractor.py ../../preprocessing/sample_results.jsonl output_triples.jsonl

# Test the implementation
python test_step3.py
```

### LLM-Powered Extraction (`llm_powered/`)
```bash
cd llm_powered/

# Install LLM dependencies
pip install -r requirements.txt

# Set API key
export OPENAI_API_KEY="your-key-here"
# OR
export ANTHROPIC_API_KEY="your-key-here"

# Estimate costs first
python test_llm_extraction.py

# Run on sample data
python extractor_llm.py llm_test_sample.jsonl output.jsonl --provider openai --batch-size 20

# Run on full data (monitor costs!)
python extractor_llm.py ../../preprocessing/sample_results.jsonl llm_output.jsonl --provider claude --batch-size 50
```

## üí∞ Cost Comparison (971 messages from sample data)

| **Approach** | **Provider** | **Batch Size** | **Estimated Cost** | **Speed** | **Accuracy** |
|--------------|--------------|----------------|-------------------|-----------|--------------|
| **Rule-Based** | Local | N/A | $0.00 | Fast | 85-90% |
| **LLM (OpenAI)** | GPT-3.5 | 20 | $0.14 | Medium | 90-95% |
| **LLM (Claude)** | Haiku | 20 | $0.06 | Fast | 90-95% |
| **LLM (OpenAI)** | GPT-3.5 | 50 | $0.14 | Faster | 85-90% |

## üìä Output Comparison

Both extractors produce the same output format:

```json
{
  "subject": "user123",
  "predicate": "asks_about", 
  "object": "DCA strategies for TQQQ",
  "message_id": "1234567890",
  "segment_id": "segment-abc123",
  "timestamp": "2024-01-01T10:00:00+00:00",
  "confidence": 0.85
}
```

**Rule-Based Results (971 messages):**
- Total triples: 1,014
- Average confidence: 0.805
- Processing time: ~1 second

**LLM Results (estimated):**
- Total triples: 1,200-1,500 (higher recall)
- Average confidence: 0.82-0.90
- Processing time: 2-5 minutes

## üéØ When to Use Which

### Use **Rule-Based** (`extractor.py`) when:
- ‚úÖ Budget is primary concern
- ‚úÖ Messages follow predictable patterns
- ‚úÖ Need fast processing (real-time)
- ‚úÖ Working with financial/trading terminology
- ‚úÖ High volume processing

### Use **LLM-Powered** (`extractor_llm.py`) when:
- ‚úÖ Accuracy is primary concern
- ‚úÖ Complex reasoning required
- ‚úÖ Abstract content (analysis, opinions)
- ‚úÖ Small to medium datasets
- ‚úÖ Budget allows for API costs

## üîß Configuration Options

### Rule-Based Extractor
- Edit patterns in `MessageTypeExtractor` class
- Modify entity normalization in `EntityNormalizer`
- Adjust confidence scores per extraction type

### LLM Extractor
- `--provider`: Choose "openai" or "claude"
- `--model`: Specific model (gpt-4, claude-3-sonnet, etc.)
- `--batch-size`: Messages per API call (10-50)
  - **10-20**: Highest accuracy, slower, more expensive
  - **20-30**: Balanced (recommended)
  - **50+**: Fastest, cheaper, may lose context

## üß™ Testing Workflow

1. **Start with cost estimation:**
   ```bash
   python test_llm_extraction.py
   ```

2. **Test on sample data:**
   ```bash
   python extractor_llm.py llm_test_sample.jsonl test_output.jsonl --provider claude
   ```

3. **Compare with rule-based:**
   ```bash
   python test_step3.py
   ```

4. **Scale to full dataset:**
   ```bash
   # Monitor costs during execution
   python extractor_llm.py ../preprocessing/sample_results.jsonl full_output.jsonl --provider claude --batch-size 30
   ```

## üìà Performance Optimization

### For Rule-Based:
- Pre-compile regex patterns
- Use batch processing for large files
- Add more entity aliases for your domain

### For LLM:
- Use Claude Haiku for cost efficiency
- Batch messages by segment for context
- Monitor rate limits
- Cache results to avoid re-processing

## üîç Quality Assessment

Both extractors include confidence scores:
- **0.9+**: High confidence (direct patterns, API results)
- **0.8-0.9**: Good confidence (pattern matches, LLM reasoning)  
- **0.6-0.8**: Medium confidence (heuristic matches)
- **<0.6**: Low confidence (fallback extractions)

## üöß Troubleshooting

### Rule-Based Issues:
- Low extraction rate? ‚Üí Add more patterns for your data
- Missing entities? ‚Üí Update entity normalization
- Wrong classifications? ‚Üí Check Step 2 preprocessing

### LLM Issues:
- API errors? ‚Üí Check rate limits and keys
- High costs? ‚Üí Reduce batch size or use Claude Haiku
- Poor accuracy? ‚Üí Reduce batch size for more context
- Timeout issues? ‚Üí Add retry logic and rate limiting

## üìÅ File Structure

```
extraction/
‚îú‚îÄ‚îÄ README.md                     # Technical specification
‚îú‚îÄ‚îÄ USAGE.md                      # This usage guide
‚îú‚îÄ‚îÄ rule_based/                   # Rule-based implementation
‚îÇ   ‚îú‚îÄ‚îÄ extractor.py              # Main rule-based extractor
‚îÇ   ‚îú‚îÄ‚îÄ test_step3.py             # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt          # Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Rule-based specific docs
‚îî‚îÄ‚îÄ llm_powered/                  # LLM-powered implementation
    ‚îú‚îÄ‚îÄ extractor_llm.py          # Main LLM extractor
    ‚îú‚îÄ‚îÄ test_llm_extraction.py    # Cost estimation & testing
    ‚îú‚îÄ‚îÄ requirements.txt          # LLM dependencies
    ‚îî‚îÄ‚îÄ README.md                 # LLM-specific docs
```

## üîÆ Future Enhancements

- **Hybrid approach**: Use LLM for complex messages, rules for simple ones
- **Active learning**: Improve rule patterns based on LLM outputs
- **Caching**: Store LLM results to avoid re-processing
- **Streaming**: Process large datasets in chunks
- **Evaluation**: Automated accuracy metrics against ground truth